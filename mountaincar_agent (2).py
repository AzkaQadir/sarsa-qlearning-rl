# -*- coding: utf-8 -*-
"""MountainCar_Agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E67g-Dpq-feZKYHux6kXtamauXQmj8fC
"""

!pip install swig
!pip install gymnasium
!pip install --upgrade gym
!pip install moviepy > /dev/null

import numpy as np
import gymnasium as gym
import matplotlib.pyplot as plt
from tqdm import tqdm
import os, base64
from gymnasium.wrappers import RecordVideo
from IPython.display import HTML

# Hyperparameters
ALPHA = 0.1
GAMMA = 0.95
EPSILON = 0.2
EPISODES = 500

"""## SARSA (0)"""

# Environment setup
env = gym.make('MountainCar-v0', render_mode='rgb_array')

# Discretization bins
position_bins = np.linspace(-1.2, 0.6, 20)
velocity_bins = np.linspace(-0.07, 0.07, 20)

def discretize_state(state):
    pos, vel = state
    pos_bin = np.clip(np.digitize(pos, position_bins) - 1, 0, len(position_bins) - 1)
    vel_bin = np.clip(np.digitize(vel, velocity_bins) - 1, 0, len(velocity_bins) - 1)
    return (pos_bin, vel_bin)

# Q-table initialization
state_space_size = (len(position_bins), len(velocity_bins))
action_space_size = env.action_space.n
Q = np.random.uniform(low=-1, high=1, size=state_space_size + (action_space_size,))

# Epsilon-greedy policy
def epsilon_greedy_policy(state, epsilon):
    return env.action_space.sample() if np.random.rand() < epsilon else np.argmax(Q[state])

# Reward shaping
def shaped_reward(state, reward):
    position, _ = state
    return reward + abs(position + 0.5)  # Encourage movement to the right

# Training loop
returns, steps_per_episode = [], []

for episode in tqdm(range(EPISODES), desc="Training"):
    state, _ = env.reset()
    state = discretize_state(state)
    action = epsilon_greedy_policy(state, EPSILON)
    done, total_reward, steps = False, 0, 0

    while not done:
        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        next_state_discrete = discretize_state(next_state)
        next_action = epsilon_greedy_policy(next_state_discrete, EPSILON)

        # SARSA(0) update with reward shaping
        shaped = shaped_reward(next_state, reward)
        td_target = shaped + GAMMA * Q[next_state_discrete][next_action] * (not done)
        Q[state][action] += ALPHA * (td_target - Q[state][action])

        state, action = next_state_discrete, next_action
        total_reward += shaped
        steps += 1

    returns.append(total_reward)
    steps_per_episode.append(steps)

env.close()

def test_policy(env, Q, episodes=10):
    total_steps = 0
    successes = 0

    for _ in range(episodes):
        state, _ = env.reset()
        state = discretize_state(state)
        done = False
        steps = 0

        while not done and steps < 500:
            action = np.argmax(Q[state])
            next_state, _, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            state = discretize_state(next_state)
            steps += 1

        total_steps += steps
        if steps < 500:
            successes += 1

    avg_steps = total_steps / episodes
    success_rate = successes / episodes
    return avg_steps, success_rate

# Plotting
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(returns)
plt.title("Returns per Episode")
plt.xlabel("Episode")
plt.ylabel("Return")

plt.subplot(1, 2, 2)
plt.plot(steps_per_episode)
plt.title("Steps per Episode")
plt.xlabel("Episode")
plt.ylabel("Steps")

plt.tight_layout()
plt.show()

# Final testing
avg_steps, success_rate = test_policy(env, Q)
print(f"\nAverage steps to goal: {avg_steps:.2f}")

import imageio.v2 as imageio  # <-- Ensure this import is here
from IPython.display import HTML
from base64 import b64encode

# Record a video of the trained agent
env = gym.make("MountainCar-v0", render_mode="rgb_array")
state, _ = env.reset()
video_frames = []
done = False
state_disc = discretize_state(state)
action = np.argmax(Q[state_disc])
steps = 0

while not done and steps < 500:
    frame = env.render()
    video_frames.append(frame)
    next_state, _, terminated, truncated, _ = env.step(action)
    done = terminated or truncated
    state_disc = discretize_state(next_state)
    action = np.argmax(Q[state_disc])
    steps += 1

env.close()

# Save and display the video
video_path = "/content/mountain_car_sarsa.mp4"
imageio.mimsave(video_path, video_frames, fps=30)

mp4 = open(video_path, 'rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
HTML(f'<video width=640 controls><source src="{data_url}" type="video/mp4"></video>')

"""# Q-learning"""

import numpy as np
import gymnasium as gym
import matplotlib.pyplot as plt
from tqdm import tqdm
import imageio
import os

# Hyperparameters
ALPHA = 0.1
GAMMA = 0.95
EPSILON = 0.2
EPISODES = 500

# Environment setup
env = gym.make('MountainCar-v0')

# Discretization of continuous state space
position_bins = np.linspace(-1.2, 0.6, 20)
velocity_bins = np.linspace(-0.07, 0.07, 20)

def discretize_state(state):
    position, velocity = state
    pos_bin = np.digitize(position, position_bins) - 1
    vel_bin = np.digitize(velocity, velocity_bins) - 1
    pos_bin = np.clip(pos_bin, 0, len(position_bins) - 1)
    vel_bin = np.clip(vel_bin, 0, len(velocity_bins) - 1)
    return (pos_bin, vel_bin)

# Initialize Q-table
state_space_size = (len(position_bins), len(velocity_bins))
action_space_size = env.action_space.n
Q = np.random.uniform(low=-1, high=1, size=state_space_size + (action_space_size,))

# Epsilon-greedy policy
def epsilon_greedy_policy(state, epsilon):
    if np.random.random() < epsilon:
        return env.action_space.sample()
    else:
        return np.argmax(Q[state])

# Reward shaping
def shaped_reward(state, original_reward):
    position, _ = state
    return original_reward + abs(position + 0.5)

# Q-Learning Training Loop
returns = []
steps_per_episode = []

for episode in tqdm(range(EPISODES)):
    state, _ = env.reset()
    state = discretize_state(state)
    total_reward = 0
    steps = 0
    done = False

    while not done:
        action = epsilon_greedy_policy(state, EPSILON)
        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        next_state_discrete = discretize_state(next_state)

        # Apply reward shaping
        reward = shaped_reward(next_state, reward)

        # Q-learning update
        td_target = reward + GAMMA * np.max(Q[next_state_discrete]) * (not done)
        td_error = td_target - Q[state][action]
        Q[state][action] += ALPHA * td_error

        state = next_state_discrete
        total_reward += reward
        steps += 1

    returns.append(total_reward)
    steps_per_episode.append(steps)

env.close()

# Plotting
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.title('Episode Returns (Q-Learning)')
plt.plot(returns)
plt.xlabel('Episode')
plt.ylabel('Total Reward')

plt.subplot(1, 2, 2)
plt.title('Steps to Goal')
plt.plot(steps_per_episode)
plt.xlabel('Episode')
plt.ylabel('Steps')

plt.tight_layout()
plt.show()

# Define test function to simulate learned strategy
def test_policy(env, Q, episodes=10):
    total_steps = 0
    successes = 0
    for _ in range(episodes):
        state, _ = env.reset()
        state = discretize_state(state)
        done = False
        steps = 0
        while not done and steps < 500:
            action = np.argmax(Q[state])
            next_state, _, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            state = discretize_state(next_state)
            steps += 1
        total_steps += steps
        if steps < 500:
            successes += 1
    avg_steps = total_steps / episodes
    success_rate = successes / episodes
    return avg_steps, success_rate

# Evaluate learned policy
env = gym.make("MountainCar-v0")
avg_steps, success_rate = test_policy(env, Q)
print(f"\nAverage steps to goal: {avg_steps:.2f}")
print(f"Success rate over 10 episodes: {success_rate*100:.1f}%")

import imageio.v2 as imageio
from IPython.display import HTML
from base64 import b64encode
import numpy as np

# Generate video
video_frames = []
state, _ = env.reset()
done = False

while not done:
    frame = env.render()  # Render without mode
    if frame is not None:  # Check for None before appending
        video_frames.append(frame)
    state_discrete = discretize_state(state)
    action = np.argmax(Q[state_discrete])
    state, _, terminated, truncated, _ = env.step(action)
    done = terminated or truncated

env.close()

# Save video
video_path = "/tmp/mountaincar_qlearning.gif"
imageio.mimsave(video_path, video_frames, fps=30)

from IPython.display import Image
Image(filename=video_path)