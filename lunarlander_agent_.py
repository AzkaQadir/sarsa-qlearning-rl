# -*- coding: utf-8 -*-
"""LunarLander_Agent .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E2iA4whQnCWmeU4JWMC21t6TdK0XoP0W
"""

!pip install swig
!pip install gymnasium
!pip install --upgrade gym
!pip install "gymnasium[box2d]"

"""## SARSA (0)"""

import numpy as np
import gymnasium as gym
import matplotlib.pyplot as plt
from tqdm import tqdm

# Hyperparameters
ALPHA = 0.1
GAMMA = 0.95
EPSILON = 0.2
EPISODES = 2000

# Discretization bins
position_bins = np.linspace(-1.5, 1.5, 10)
velocity_bins = np.linspace(-3.0, 3.0, 10)
angle_bins = np.linspace(-0.4, 0.4, 10)
angular_velocity_bins = np.linspace(-3.0, 3.0, 10)
leg_contact_bins = [0, 1]

# Discretize Lunar Lander state (8D ‚Üí reduced for simplicity)
def discretize_state(state):
    pos_x = np.digitize(state[0], position_bins)
    pos_y = np.digitize(state[1], position_bins)
    vel_x = np.digitize(state[2], velocity_bins)
    vel_y = np.digitize(state[3], velocity_bins)
    angle = np.digitize(state[4], angle_bins)
    ang_vel = np.digitize(state[5], angular_velocity_bins)
    left_leg = int(state[6])
    right_leg = int(state[7])

    return (pos_x, pos_y, vel_x, vel_y, angle, ang_vel, left_leg, right_leg)

# Initialize environment
env = gym.make("LunarLander-v3")

# Q-table shape: (state_space) + (action_space,)
state_space_size = (11, 11, 11, 11, 11, 11, 2, 2)
action_space_size = env.action_space.n
Q = np.random.uniform(low=-1, high=1, size=state_space_size + (action_space_size,))

# Epsilon-greedy policy
def epsilon_greedy(Q, state, epsilon):
    if np.random.rand() < epsilon:
        return env.action_space.sample()
    return np.argmax(Q[state])

# SARSA(0) training loop
returns = []
for episode in tqdm(range(EPISODES), desc="Training SARSA"):
    state_raw, _ = env.reset()
    state = discretize_state(state_raw)
    action = epsilon_greedy(Q, state, EPSILON)
    total_reward = 0
    done = False

    while not done:
        next_state_raw, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        next_state = discretize_state(next_state_raw)
        next_action = epsilon_greedy(Q, next_state, EPSILON)

        # SARSA update
        td_target = reward + GAMMA * Q[next_state][next_action] * (not done)
        td_error = td_target - Q[state][action]
        Q[state][action] += ALPHA * td_error

        state = next_state
        action = next_action
        total_reward += reward

    returns.append(total_reward)

env.close()

# Moving average
def moving_average(data, window_size=50):
    return np.convolve(data, np.ones(window_size) / window_size, mode='valid')

plt.figure(figsize=(10, 5))
plt.plot(returns, alpha=0.3, label='Episode Return')
plt.plot(moving_average(returns), label='Moving Average (50 episodes)', color='red')
plt.xlabel("Episode")
plt.ylabel("Return")
plt.title("SARSA(0) on LunarLander-v3: Episode Returns")
plt.legend()
plt.grid(True)
plt.show()

import os
import base64
from IPython.display import HTML
from gymnasium.wrappers import RecordVideo

def simulate_policy(Q):
    video_folder = "lander_videos"
    if os.path.exists(video_folder):
        for f in os.listdir(video_folder):
            os.remove(os.path.join(video_folder, f))
    else:
        os.makedirs(video_folder)

    sim_env = gym.make("LunarLander-v3", render_mode="rgb_array")
    sim_env = RecordVideo(sim_env, video_folder=video_folder, episode_trigger=lambda e: True)

    state_raw, _ = sim_env.reset()
    state = discretize_state(state_raw)
    done = False
    steps = 0

    while not done:
        action = np.argmax(Q[state])
        state_raw, _, terminated, truncated, _ = sim_env.step(action)
        state = discretize_state(state_raw)
        done = terminated or truncated
        steps += 1

    sim_env.close()
    return os.path.join(video_folder, "rl-video-episode-0.mp4"), steps

video_path, steps_taken = simulate_policy(Q)
print(f"Steps to complete goal in final policy: {steps_taken}")

def show_video(video_path):
    video_file = open(video_path, "rb").read()
    video_base64 = base64.b64encode(video_file).decode("utf-8")
    return HTML(f'<video width="640" height="480" controls><source src="data:video/mp4;base64,{video_base64}" type="video/mp4"></video>')

show_video(video_path)

"""# Q-learning"""

# Install moviepy for video display in Colab
!pip install moviepy > /dev/null

import numpy as np
import gymnasium as gym
import matplotlib.pyplot as plt
from tqdm import tqdm
import os
import base64
from IPython.display import HTML
from gymnasium.wrappers import RecordVideo

# Hyperparameters
ALPHA = 0.1
GAMMA = 0.95
EPSILON = 0.2
EPISODES = 2000

# Discretization bins
position_bins = np.linspace(-1.5, 1.5, 10)
velocity_bins = np.linspace(-3.0, 3.0, 10)
angle_bins = np.linspace(-0.4, 0.4, 10)
angular_velocity_bins = np.linspace(-3.0, 3.0, 10)

def discretize_state(state):
    pos_x = np.digitize(state[0], position_bins)
    pos_y = np.digitize(state[1], position_bins)
    vel_x = np.digitize(state[2], velocity_bins)
    vel_y = np.digitize(state[3], velocity_bins)
    angle = np.digitize(state[4], angle_bins)
    ang_vel = np.digitize(state[5], angular_velocity_bins)
    left_leg = int(state[6])
    right_leg = int(state[7])
    return (pos_x, pos_y, vel_x, vel_y, angle, ang_vel, left_leg, right_leg)

# Environment
env = gym.make("LunarLander-v3")
state_space_size = (11, 11, 11, 11, 11, 11, 2, 2)
action_space_size = env.action_space.n
Q = np.random.uniform(low=-1, high=1, size=state_space_size + (action_space_size,))

# Epsilon-greedy
def epsilon_greedy(Q, state, epsilon):
    if np.random.rand() < epsilon:
        return env.action_space.sample()
    return np.argmax(Q[state])

# Q-learning training loop
returns = []
for episode in tqdm(range(EPISODES), desc="Training Q-Learning"):
    state_raw, _ = env.reset()
    state = discretize_state(state_raw)
    total_reward = 0
    done = False

    while not done:
        action = epsilon_greedy(Q, state, EPSILON)
        next_state_raw, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        next_state = discretize_state(next_state_raw)

        # Q-learning update
        best_next_action = np.argmax(Q[next_state])
        td_target = reward + GAMMA * Q[next_state][best_next_action] * (not done)
        td_error = td_target - Q[state][action]
        Q[state][action] += ALPHA * td_error

        state = next_state
        total_reward += reward

    returns.append(total_reward)

env.close()

# Plot returns
def moving_average(data, window_size=50):
    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')

plt.figure(figsize=(10, 5))
plt.plot(moving_average(returns))
plt.xlabel('Episodes')
plt.ylabel('Total Reward (Moving Avg)')
plt.title('Q-Learning on Lunar Lander')
plt.grid(True)
plt.show()

# Simulate policy and record video
def simulate_policy(Q):
    video_folder = "videos"
    if os.path.exists(video_folder):
        for f in os.listdir(video_folder):
            os.remove(os.path.join(video_folder, f))
    else:
        os.makedirs(video_folder)

    # Create environment with no time limit for visualization
    sim_env = gym.make("MountainCar-v0", render_mode="rgb_array")
    sim_env = RecordVideo(sim_env, video_folder=video_folder,
                         episode_trigger=lambda e: True)

    state_raw = sim_env.reset()[0]
    state = discretize_state(state_raw)
    done = False
    steps = 0

    while not done and steps < 1000:  # Safety limit
        action = np.argmax(Q[state])
        state_raw, _, terminated, truncated, _ = sim_env.step(action)
        state = discretize_state(state_raw)
        done = terminated or truncated
        steps += 1

        # Check if goal was reached
        if terminated and state_raw[0] >= 0.5:
            print("Goal reached in", steps, "steps!")
            break

    sim_env.close()
    return os.path.join(video_folder, "rl-video-episode-0.mp4")

def show_video(path):
    video = open(path, "rb").read()
    b64 = base64.b64encode(video).decode()
    return HTML(f'<video width="640" height="480" controls><source src="data:video/mp4;base64,{b64}" type="video/mp4"></video>')

# Run and show
video_path, steps = simulate_policy(Q)
print(f"Final policy completed landing in {steps} steps.")
show_video(video_path)

"""## SARSA (ùúÜ)"""

import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import time

# Hyperparameters
alpha = 0.1
gamma = 0.95
epsilon = 0.2
_lambda = 0.8
n_episodes = 2000
max_steps = 1000

env = gym.make("LunarLander-v3", render_mode=None)

# Discretize the state space
def create_bins():
    bins = [
        np.linspace(-1.5, 1.5, 6),     # x position
        np.linspace(-.5, 1.5, 6),      # y position
        np.linspace(-3, 3, 6),         # x velocity
        np.linspace(-3, 3, 6),         # y velocity
        np.linspace(-np.pi, np.pi, 6),# angle
        np.linspace(-3, 3, 6),         # angular velocity
        [0, 0.5, 1],                   # left leg contact
        [0, 0.5, 1]                    # right leg contact
    ]
    return bins

bins = create_bins()

def discretize_state(state, bins):
    return tuple(np.digitize(state[i], bins[i]) for i in range(len(state)))

n_actions = env.action_space.n
q_table = {}
eligibility = {}

def choose_action(state):
    if np.random.random() < epsilon:
        return np.random.choice(n_actions)
    else:
        q_values = [q_table.get((state, a), 0.0) for a in range(n_actions)]
        return int(np.argmax(q_values))

returns = []

# Main loop
for episode in range(n_episodes):
    state, _ = env.reset()
    state_d = discretize_state(state, bins)
    action = choose_action(state_d)

    eligibility.clear()
    total_reward = 0

    for step in range(max_steps):
        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        next_state_d = discretize_state(next_state, bins)
        next_action = choose_action(next_state_d)

        q_sa = q_table.get((state_d, action), 0.0)
        q_sap = q_table.get((next_state_d, next_action), 0.0)
        delta = reward + gamma * q_sap - q_sa

        # Update eligibility trace
        eligibility[(state_d, action)] = eligibility.get((state_d, action), 0) + 1

        # Update Q-values for all state-action pairs
        for key in eligibility:
            q_table[key] = q_table.get(key, 0.0) + alpha * delta * eligibility[key]
            eligibility[key] *= gamma * _lambda

        state_d = next_state_d
        action = next_action
        total_reward += reward

        if done:
            break

    returns.append(total_reward)
    if (episode + 1) % 100 == 0:
        avg_return = np.mean(returns[-100:])
        print(f"Episode {episode+1}, Average Return (last 100): {avg_return:.2f}")

# Plotting the returns
plt.plot(returns)
plt.xlabel("Episodes")
plt.ylabel("Returns")
plt.title("SARSA(Œª) Performance on LunarLander-v2")
plt.grid()
plt.show()

def test_agent(q_table, bins, episodes=5):
    env = gym.make("LunarLander-v3", render_mode="human")
    for ep in range(episodes):
        state, _ = env.reset()
        done = False
        total_reward = 0
        steps = 0

        while not done:
            state_d = discretize_state(state, bins)
            q_vals = [q_table.get((state_d, a), 0.0) for a in range(n_actions)]
            action = np.argmax(q_vals)
            state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            total_reward += reward
            steps += 1

        print(f"Test Episode {ep+1}: Return = {total_reward}, Steps = {steps}")
    env.close()

# Simulate and time the test
start_time = time.time()
test_agent(q_table, bins)
end_time = time.time()
print(f"Total simulation time: {end_time - start_time:.2f} seconds")

!pip install gymnasium[box2d] imageio ffmpeg

import imageio
from IPython.display import HTML
from base64 import b64encode

def record_video(env, q_table, bins, video_path="lunarlander_sarsa.mp4", episode_length=1000):
    frames = []
    state, _ = env.reset()
    state_d = discretize_state(state, bins)

    for t in range(episode_length):
        frame = env.render()  # capture RGB frame
        frames.append(frame)

        q_vals = [q_table.get((state_d, a), 0.0) for a in range(n_actions)]
        action = int(np.argmax(q_vals))
        state, _, terminated, truncated, _ = env.step(action)
        done = terminated or truncated

        state_d = discretize_state(state, bins)
        if done:
            break

    # Save video using imageio
    imageio.mimsave(video_path, frames, fps=30)

def display_video(video_path="lunarlander_sarsa.mp4"):
    mp4 = open(video_path, 'rb').read()
    b64 = b64encode(mp4).decode()
    return HTML(f'<video width=640 controls><source src="data:video/mp4;base64,{b64}" type="video/mp4"></video>')

# Create a new environment with rgb_array mode
env_video = gym.make("LunarLander-v3", render_mode="rgb_array")
record_video(env_video, q_table, bins)
env_video.close()

# Display the video
display_video()